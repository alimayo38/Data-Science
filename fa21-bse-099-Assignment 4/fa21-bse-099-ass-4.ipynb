{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Suo30uqkP3LZ",
        "outputId": "0e1adb59-8493-429b-d623-41379dd23c65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Labels (Sentence Numbers):\n",
            "Document 1\n",
            "Document 2\n",
            "Document 3\n",
            "Vocabulary: ['analysis', 'best', 'computer', 'courses', 'data', 'important', 'in', 'is', 'most', 'of', 'one', 'perform', 'science', 'scientists', 'the', 'this']\n",
            "\n",
            "Document-Term Matrix (BoW representation):\n",
            "Document 1:\t[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 0, 1, 0]\n",
            "Document 2:\t[0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1]\n",
            "Document 3:\t[1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0]\n",
            "\n",
            "Term Frequency Matrix (TF values):\n",
            "Document 1 :\t[0.0, 0.0, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.0, 0.3333333333333333, 0.0, 0.16666666666666666, 0.0]\n",
            "Document 2 :\t[0.0, 0.16666666666666666, 0.0, 0.16666666666666666, 0.16666666666666666, 0.0, 0.0, 0.16666666666666666, 0.0, 0.16666666666666666, 0.16666666666666666, 0.0, 0.16666666666666666, 0.0, 0.16666666666666666, 0.16666666666666666]\n",
            "Document 3 :\t[0.16666666666666666, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.16666666666666666, 0.16666666666666666, 0.0]\n",
            "\n",
            "Inverse Document Frequency (IDF values):\n",
            "analysis\t0.477\n",
            "best\t0.477\n",
            "computer\t0.477\n",
            "courses\t0.176\n",
            "data\t0.000\n",
            "important\t0.477\n",
            "in\t0.477\n",
            "is\t0.176\n",
            "most\t0.477\n",
            "of\t0.176\n",
            "one\t0.176\n",
            "perform\t0.477\n",
            "science\t0.176\n",
            "scientists\t0.477\n",
            "the\t0.000\n",
            "this\t0.477\n",
            "\n",
            "TF-IDF values:\n",
            "Document 1:\t[0.0, 0.0, 0.07952020911994373, 0.029348543175946873, 0.0, 0.07952020911994373, 0.07952020911994373, 0.029348543175946873, 0.07952020911994373, 0.029348543175946873, 0.029348543175946873, 0.0, 0.058697086351893746, 0.0, 0.0, 0.0]\n",
            "Document 2:\t[0.0, 0.07952020911994373, 0.0, 0.029348543175946873, 0.0, 0.0, 0.0, 0.029348543175946873, 0.0, 0.029348543175946873, 0.029348543175946873, 0.0, 0.029348543175946873, 0.0, 0.0, 0.07952020911994373]\n",
            "Document 3:\t[0.07952020911994373, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07952020911994373, 0.0, 0.07952020911994373, 0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "# 12/12/2023\n",
        "# CSC461 – Assignment4 – NLP\n",
        "# ALI SHAKEEL\n",
        "# FA21-BSE-099\n",
        "# QUESTION 1 Concepts about BoW ,TF,IDF and TF-IDF ||| QUESTION 2 Concepts about cosine similarity,Manhattan distance and  Euclidean distance\n",
        "\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine, cityblock, euclidean\n",
        "\n",
        "sentences = [\n",
        "    \"data science is one of the most important courses in computer science\",\n",
        "    \"this is one of the best data science courses\",\n",
        "    \"the data scientists perform data analysis\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "vocabulary = list(set(\" \".join(sentences).split()))\n",
        "vocabulary.sort()\n",
        "\n",
        "\n",
        "document_term_matrix = []\n",
        "for sentence in sentences:\n",
        "    word_count = Counter(sentence.split())\n",
        "    row = [word_count.get(term, 0) for term in vocabulary]\n",
        "    document_term_matrix.append(row)\n",
        "\n",
        "\n",
        "term_frequency_matrix = [[count / len(sentence.split()) for count in row] for row in document_term_matrix]\n",
        "\n",
        "\n",
        "document_frequency = {term: sum(1 for row in document_term_matrix if row[i] > 0) for i, term in enumerate(vocabulary)}\n",
        "\n",
        "\n",
        "inverse_document_frequency = [math.log10(len(sentences) / document_frequency[term]) if document_frequency[term] > 0 else 0 for term in vocabulary]\n",
        "\n",
        "\n",
        "tfidf_matrix = [[tf * idf for tf, idf in zip(row, inverse_document_frequency)] for row in term_frequency_matrix]\n",
        "\n",
        "\n",
        "print(\"\\nDocument Labels (Sentence Numbers):\")\n",
        "for i in range(len(sentences)):\n",
        "    print(f\"Document {i + 1}\")\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"\\nDocument-Term Matrix (BoW representation):\")\n",
        "for i, row in enumerate(document_term_matrix):\n",
        "    print(f\"Document {i + 1}:\\t{row}\")\n",
        "i = 1\n",
        "print(\"\\nTerm Frequency Matrix (TF values):\")\n",
        "for term, row in zip(vocabulary, term_frequency_matrix):\n",
        "    if i == 1 or i == 2 or i == 3:\n",
        "        print(f\"Document {i} :\\t{row}\")\n",
        "        i += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nInverse Document Frequency (IDF values):\")\n",
        "for term, idf in zip(vocabulary, inverse_document_frequency):\n",
        "    print(f\"{term}\\t{idf:.3f}\")\n",
        "\n",
        "print(\"\\nTF-IDF values:\")\n",
        "for i, row in enumerate(tfidf_matrix):\n",
        "    print(f\"Document {i + 1}:\\t{row}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2\n",
        "\n",
        "\n",
        "\n",
        "document_term_matrix = []\n",
        "for sentence in sentences:\n",
        "    word_count = Counter(sentence.split())\n",
        "    row = [word_count.get(term, 0) for term in vocabulary]\n",
        "    document_term_matrix.append(row)\n",
        "\n",
        "\n",
        "doc_term_matrix_array = np.array(document_term_matrix)\n",
        "\n",
        "#  cosine similarity\n",
        "cosine_sim_s1_s2 = 1 - cosine(doc_term_matrix_array[0], doc_term_matrix_array[1])\n",
        "cosine_sim_s1_s3 = 1 - cosine(doc_term_matrix_array[0], doc_term_matrix_array[2])\n",
        "cosine_sim_s2_s3 = 1 - cosine(doc_term_matrix_array[1], doc_term_matrix_array[2])\n",
        "\n",
        "# Manhattan distance\n",
        "manhattan_dist_s1_s2 = cityblock(doc_term_matrix_array[0], doc_term_matrix_array[1])\n",
        "manhattan_dist_s1_s3 = cityblock(doc_term_matrix_array[0], doc_term_matrix_array[2])\n",
        "manhattan_dist_s2_s3 = cityblock(doc_term_matrix_array[1], doc_term_matrix_array[2])\n",
        "\n",
        "#  Euclidean distance\n",
        "euclidean_dist_s1_s2 = euclidean(doc_term_matrix_array[0], doc_term_matrix_array[1])\n",
        "euclidean_dist_s1_s3 = euclidean(doc_term_matrix_array[0], doc_term_matrix_array[2])\n",
        "euclidean_dist_s2_s3 = euclidean(doc_term_matrix_array[1], doc_term_matrix_array[2])\n",
        "\n",
        "\n",
        "print(\"Cosine Similarity:\")\n",
        "print(f\"S1 and S2: {cosine_sim_s1_s2:.4f}\")\n",
        "print(f\"S1 and S3: {cosine_sim_s1_s3:.4f}\")\n",
        "print(f\"S2 and S3: {cosine_sim_s2_s3:.4f}\")\n",
        "\n",
        "print(\"\\nManhattan Distance:\")\n",
        "print(f\"S1 and S2: {manhattan_dist_s1_s2}\")\n",
        "print(f\"S1 and S3: {manhattan_dist_s1_s3}\")\n",
        "print(f\"S2 and S3: {manhattan_dist_s2_s3}\")\n",
        "\n",
        "print(\"\\nEuclidean Distance:\")\n",
        "print(f\"S1 and S2: {euclidean_dist_s1_s2:.4f}\")\n",
        "print(f\"S1 and S3: {euclidean_dist_s1_s3:.4f}\")\n",
        "print(f\"S2 and S3: {euclidean_dist_s2_s3:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04DwViShW3aV",
        "outputId": "d7efc1c6-c2eb-4c76-9f2c-cd3e023b8339"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity:\n",
            "S1 and S2: 0.7127\n",
            "S1 and S3: 0.2835\n",
            "S2 and S3: 0.3536\n",
            "\n",
            "Manhattan Distance:\n",
            "S1 and S2: 7\n",
            "S1 and S3: 14\n",
            "S2 and S3: 11\n",
            "\n",
            "Euclidean Distance:\n",
            "S1 and S2: 2.6458\n",
            "S1 and S3: 4.0000\n",
            "S2 and S3: 3.3166\n"
          ]
        }
      ]
    }
  ]
}